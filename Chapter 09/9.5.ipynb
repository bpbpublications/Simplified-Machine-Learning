import numpy as np

def value_iteration(S, A, P, R, gamma, epsilon=1e-6, max_iterations=1000):
    # Initialize value function
    V = np.zeros(len(S))

    # Perform value iteration
    for _ in range(max_iterations):
        V_old = np.copy(V)
        for s in S:
            # Calculate the value for state s using Bellman equation
            V[s] = max([sum([P[s, a, s1] * (R[s, a, s1] + gamma * V_old[s1]) for s1 in S]) for a in A])

        # Check for convergence
        if np.max(np.abs(V - V_old)) < epsilon:
            break

    # Extract optimal policy
    policy = {}
    for s in S:
        policy[s] = np.argmax([sum([P[s, a, s1] * (R[s, a, s1] + gamma * V[s1]) for s1 in S]) for a in A])

    return V, policy

# Example MDP
S = [0, 1]  # States
A = [0, 1]  # Actions
P = {(0, 0, 0): 0.5, (0, 0, 1): 0.5, (0, 1, 0): 0.1, (0, 1, 1): 0.9,
     (1, 0, 0): 0.2, (1, 0, 1): 0.8, (1, 1, 0): 0.4, (1, 1, 1): 0.6}  # Transition probabilities
R = {(0, 0, 0): 0, (0, 0, 1): 1, (0, 1, 0): 2, (0, 1, 1): 3,
     (1, 0, 0): 0, (1, 0, 1): 1, (1, 1, 0): 2, (1, 1, 1): 3}  # Rewards
gamma = 0.9  # Discount factor

# Solve MDP using value iteration
V, policy = value_iteration(S, A, P, R, gamma)

# Print results
print("Optimal Value Function:")
print(V)
print("Optimal Policy:")
print(policy)

 
