import numpy as np

# Define the environment (grid-world)
# 'S' is the starting point, 'G' is the goal, 'H' is the hole, 'F' is the frozen surface
env = [
    ['S', 'F', 'F', 'F'],
    ['F', 'H', 'F', 'H'],
    ['F', 'F', 'F', 'H'],
    ['H', 'F', 'F', 'G']
]

# Define parameters
alpha = 0.1  # Learning rate
gamma = 0.99  # Discount factor
epsilon = 0.1  # Epsilon-greedy exploration parameter
num_episodes = 1000  # Number of episodes

# Initialize Q-table
num_states = len(env) * len(env[0])
num_actions = 4  # Up, Down, Left, Right
Q = np.zeros((num_states, num_actions))

# Helper function to convert (x, y) coordinates to state index
def state_index(x, y):
    return x * len(env[0]) + y

# Helper function to choose an action using epsilon-greedy strategy
def choose_action(state):
    if np.random.uniform(0, 1) < epsilon:
        return np.random.choice(num_actions)  # Explore
    else:
        return np.argmax(Q[state, :])  # Exploit

# Q-learning algorithm
for episode in range(num_episodes):
    state = state_index(0, 0)  # Initial state
    done = False
    while not done:
        action = choose_action(state)
        next_state = None
        if action == 0:  # Up
            next_state = state_index(max(state // len(env[0]) - 1, 0), state % len(env[0]))
        elif action == 1:  # Down
            next_state = state_index(min(state // len(env[0]) + 1, len(env) - 1), state % len(env[0]))
        elif action == 2:  # Left
            next_state = state_index(state // len(env[0]), max(state % len(env[0]) - 1, 0))
        else:  # Right
            next_state = state_index(state // len(env[0]), min(state % len(env[0]) + 1, len(env[0]) - 1))
        
        reward = 0
        if env[next_state // len(env[0])][next_state % len(env[0])] == 'H':
            reward = -1  # Penalty for falling into the hole
            done = True
        elif env[next_state // len(env[0])][next_state % len(env[0])] == 'G':
            reward = 1  # Reward for reaching the goal
            done = True

        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])
        state = next_state

print("Q-values:")
print(Q)


 
