import math

# Define a toy dataset (features and labels)
data = [
    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': False, 'play_tennis': False},
    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': True, 'play_tennis': False},
    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': False, 'play_tennis': True},
    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': False, 'play_tennis': True},
    # ... (more data points)
]

# Function to calculate entropy
def calculate_entropy(data):
    num_data = len(data)
    label_counts = {}
    for record in data:
        label = record['play_tennis']
        if label not in label_counts:
            label_counts[label] = 0
        label_counts[label] += 1
    entropy = 0.0
    for label in label_counts:
        prob = label_counts[label] / num_data
        entropy -= prob * math.log(prob, 2)
    return entropy

# Function to split the dataset based on a feature
def split_dataset(data, feature):
    subsets = {}
    for record in data:
        value = record[feature]
        if value not in subsets:
            subsets[value] = []
        subsets[value].append(record)
    return subsets

# Function to select the best feature to split on
def select_best_feature(data, features):
    base_entropy = calculate_entropy(data)
    best_info_gain = 0.0
    best_feature = None
    for feature in features:
        feature_values = set([record[feature] for record in data])
        new_entropy = 0.0
        for value in feature_values:
            subset = [record for record in data if record[feature] == value]
            prob = len(subset) / len(data)
            new_entropy += prob * calculate_entropy(subset)
        info_gain = base_entropy - new_entropy
        if info_gain > best_info_gain:
            best_info_gain = info_gain
            best_feature = feature
    return best_feature

# Function to build the decision tree
def build_tree(data, features):
    class_labels = [record['play_tennis'] for record in data]
    if class_labels.count(class_labels[0]) == len(class_labels):
        return class_labels[0]  # Return a leaf node
    if len(features) == 0:
        return max(set(class_labels), key=class_labels.count)  # Return the majority class
    best_feature = select_best_feature(data, features)
    tree = {best_feature: {}}
    feature_values = set([record[best_feature] for record in data])
    for value in feature_values:
        sub_features = features.copy()
        sub_features.remove(best_feature)
        subset = [record for record in data if record[best_feature] == value]
        tree[best_feature][value] = build_tree(subset, sub_features)
    return tree

# Main function to build and print the decision tree
def main():
    features = list(data[0].keys())  # List of features
    features.remove('play_tennis')  # Remove the target variable
    tree = build_tree(data, features)
    print(tree)

if __name__ == '__main__':
    main()


 
